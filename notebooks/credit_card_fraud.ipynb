{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is and End-to-End ML project on credit card fraud detection. The data set used is freely available [on Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). Here some EDA will be done and later several models will be tested out. The main challenge here is a huge imbalance in the data, hence some techniques such as cost-sensetive learning and resampling will be tried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with importing required libraries and then take a first look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, balanced_accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, RFE\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SVMSMOTE, KMeansSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import imblearn.pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import hyperopt \n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import joblib\n",
    "import time\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the data just seems like some random numbers. Indeed, feature names (except for `Time`, `Amount` and `Class`) don't seem to make sence. There's a reason for that: according to dataset description on Kaggle, the dataset contains transactions made by credit cards in September 2013 by European cardholders and there are s only numerical input variables which are the result of a PCA ([Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)) transformation. So, rather than working with original features, due to confidentiality issues we have to work with features derived using PCA.\n",
    "\\\n",
    "`Time` contains the seconds elapsed between each transaction and the first transaction in the dataset. `Amount` is the transaction Amount, and `Class` is the target variable: 1=Fraud, 0=Non-Fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 280K+ transactions in the dataset, What about Data Types and Null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all data is numeric, there will be no need to encoding. But it was obvious, as PCA returns only numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, no Nulls at all! Then we can just move on to EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all features except for `Time`, `Amount` and `Class` are PCA-generated, I don't think we could find anything interesting in their descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()[['Time', 'Amount', 'Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at mean of `Class` being very close to 0 we can already see a problem: the target value is very imbalanced. Let's explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vals=pd.DataFrame({'Total Count':df['Class'].value_counts()})\n",
    "target_vals['Total %']=target_vals['Total Count'].map(lambda x: x/df.shape[0]*100)\n",
    "target_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is: a highly imbalanced problem. Here, out of 284807 samples, there are only 492 cases of fraud, less than 0.18%! This problem must be tackled during modelling part of the project, using various techniques (resampling, changing class weights, etc).\n",
    "Now let's look at differences between fraudulent and nirmal transactions in-detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud, normal=df[df['Class']==1], df[df['Class']==0]\n",
    "fraud.shape, normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_subplots(rows=2, cols=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=fraud['Amount'], nbinsx=100), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=fraud['Time'], nbinsx=50), row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=600, width=1200, title_text=\"Amount and Time Distribution (Fraudulent Transactions)\")\n",
    "fig.update_xaxes(title_text=\"Amount ($)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_subplots(rows=2, cols=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=normal['Amount'], nbinsx=100), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=normal['Time'], nbinsx=50), row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=600, width=1200, title_text=\"Amount and Time Distribution (Normal Transactions)\")\n",
    "fig.update_xaxes(title_text=\"Amount ($)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Time\", row=2, col=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no that much of a difference in Amount distributions, but Time is indeed differs, as fraudulent transactions are more uniformly distributed, while normal transactions have a cyclical distribution. Now what's about descriptive statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data=fraud.describe()[['Time', 'Amount']]\n",
    "normal_data=normal.describe()[['Time', 'Amount']]\n",
    "compare=pd.concat([fraud_data, normal_data], axis=1)\n",
    "compare.columns=['Fraud_Time', 'Fraud_Amount', 'Normal_Time', 'Normal_Amount']\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that the average amount of fraudulent transaction is 1.4x higher compared to Normal ones. Std is almost the same. \n",
    "\\\n",
    "Now let's compare PCA-generated features and look for some valuable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_features=df.columns.drop(['Time', 'Amount', 'Class'])\n",
    "v_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 14*4))\n",
    "gs = gridspec.GridSpec(ncols=2, nrows=14)\n",
    "for i, cn in enumerate(df[v_features]):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(df[cn][df['Class']==1], bins=50)\n",
    "    sns.distplot(df[cn][df['Class']==0], bins=50)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(f'histogram of feature: {str(cn)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of plots here, but there is some valuable information here. Features V1-V7, V9-V12, V14, V16-V19 and V21 of classes 0 and 1 seem to have different distributions. It means they could help our future ML model to distinquish classes (we'll test this claim while evaluating feature importance). Other features are pretty much the same. One could even argue in favor of dropping them, but I won't do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing is to find Pearson's correlation between features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['Class'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V11, V4 and V2 have some positive correlation with `Class`. V17, V14 and V12 have medium negative correlations. As we don't have very strong correlation (0.8+) here, there's no need to drop anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I'll scale non-PCA features using sklearn's `RobustScaler`. As for Feature Engineering, there's not that much one can do here manually (again, because of artificial PCA-created features). Some automatic FE can be tried using `PolynomialFeatures` or `featureloots`. Later, feature selection can be performed to get most useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=df.drop(columns=['Class']), df['Class']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X, y, stratify=y)\n",
    "\n",
    "for y_split in (y_train, y_test):\n",
    "    zeros, ones=np.unique(y_split, return_counts=True)[1]\n",
    "    print(zeros/(zeros+ones)*100, ones/(zeros+ones)*100) # see the share of train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to use stratified split to preserve class imbalance in two sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use abovementioned `PolynomialFeatures`. What it does is creating polynomial and interaction features. The idea here is, given a feature(s) with some influence on target, generate even stronger features by raising them to the power 2 or 3 , multiply them, etc. But one must be carefull, as the number of features can explode very easily. I'll use degree of 2, so I won't get that much of new features. Also, I'll save a copy of original data to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig=X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly=PolynomialFeatures(2)\n",
    "X_train_poly=poly.fit_transform(X_train)\n",
    "X_test_poly=poly.transform(X_test)\n",
    "\n",
    "X_train_poly.shape, X_test_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from 30 to almost 500 features. Now, let's see some new features and later fit original and transformed data to simple `LogisticRegression` classifier and see results, For evaluation, ROC AUC Score is used, as simple accuracy for imbalanced data is not a good option. \n",
    "\\\n",
    "(NOTE: No Scaling (for `Amount` and `Time`) was used, but was tested out. Surprisingly, it gave slightly worse results. It [can happen](https://stackoverflow.com/questions/26668316/effect-of-feature-scaling-on-accuracy) sometimes. My guess, it's because of mixing non-scaled PCA features with scaled non-PCA ones, but it may be something else too.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train(xtrain, xtest, ytrain, ytest, data_type):\n",
    "    lr=LogisticRegression()\n",
    "    start_time=time.time()\n",
    "    lr.fit(xtrain, ytrain)\n",
    "    roc=roc_auc_score(ytest, lr.predict(xtest))\n",
    "    print(f'({data_type}) ROC AUC Score: {roc}')\n",
    "    print(f'Time spent: {time.time()-start_time}')\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(orig) ROC AUC Score: 0.853327918540655\n",
      "Time spent: 2.272566318511963\n",
      "(poly) ROC AUC Score: 0.8857990027585246\n",
      "Time spent: 22.178084135055542\n"
     ]
    }
   ],
   "source": [
    "for data_type, x_tr, x_te in ('orig', X_train_orig, X_test_orig), ('poly', X_train_poly, X_test_poly):\n",
    "    simple_train(x_tr, x_te, y_train, y_test, data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there's clearly some performance boost. But there's a problem: much longer training time. It's not a surprise, as we got much more features to deal with. Some feature selection techniques should be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection is the process of reducing the number of input variables when developing an ML model. It can help us to get rid of non-informative or redundant features. One ca use many ways to do FS: Correlation, RFE, Feature Importance, etc. I'll use statictical methods (namely, [ANOVA F measure](https://en.wikipedia.org/wiki/F-test)) to select subsets of features based on their relationship with the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213605, 347)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs=SelectPercentile(score_func=f_classif, percentile=70)\n",
    "X_train_poly_fs=fs.fit_transform(X_train_poly, y_train)\n",
    "X_test_poly_fs=fs.transform(X_test_poly)\n",
    "X_train_poly_fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(poly_fs) ROC AUC Score: 0.868342987654753\n",
      "Time spent: 10.47500205039978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.868342987654753"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train(X_train_poly_fs, X_test_poly_fs, y_train, y_test, 'poly_fs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've dropped 30% of features and still get performance boost. Pretty good if you ask me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build an end-to-end pipeline for all transformations using `Pipeline`. This is for the sake of demonstration. Later a final pipeline will be constructed and used in deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868342987654753"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline=Pipeline([\n",
    "                ('polynom', PolynomialFeatures(2)),\n",
    "                ('feature_select', SelectPercentile(score_func=f_classif, percentile=70)),\n",
    "                ('linear_regression', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is (in my opinion) the most fascinating part of this project. That's because here the imbalance problem of the dataset will be tackled. Various methods and models will be used and one will be chosen for later optimization and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced learning techniques can be broadly categorized into cost-sensitive and resampling methods. In **cost-sensitive** methods, algorithms are adjusted to favor the detection of the minority class. This usually implies a modification of the optimization function in the training step of the learning algorithm. \n",
    "\\\n",
    "On the contrary, **resampling methods** operate at the data level, by adding a pre-processing step to rebalance the dataset before the training algorithm is applied. Resampling can be achieved by removing examples of the majority class (undersampling techniques), adding examples of the minority class (oversampling techniques), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-Sensitive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost-sensitive learning is a subfield of machine learning that addresses classification problems where the misclassification costs are not equal. Here an imbalance ratio (IR) is used to make a model penalize minority-class mistakes more. In sklearn we can use `class_weight` to make the model cost-sensitive. To get an IR we just do minority class ratio\\majory class ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((284807, 30), (284807,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001727485630620034 0.9982725143693799\n",
      "0.00173047500131896\n"
     ]
    }
   ],
   "source": [
    "minor_share, major_share=np.unique(y, return_counts=True)[1][1]/df.shape[0], np.unique(y, return_counts=True)[1][0]/df.shape[0]\n",
    "print(minor_share, major_share)\n",
    "\n",
    "IR=minor_share/major_share\n",
    "print(IR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare 2 `LogisticRegression` models, one default and another cost-sensitive. First, we must get our transformed X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213605, 347)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_feats=PolynomialFeatures(2)\n",
    "feat_select=SelectPercentile(score_func=f_classif, percentile=70)\n",
    "\n",
    "data_pipe=Pipeline([\n",
    "                ('polynom', poly_feats),\n",
    "                ('feature_select', feat_select)])\n",
    "\n",
    "X_transformed=data_pipe.fit_transform(X_train, y_train)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll save this pipeline for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/data_pipe.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(data_pipe, '../models/data_pipe.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_cv(X, y, class_weight={0:1,1:1}):\n",
    "    model=LogisticRegression(class_weight=class_weight)\n",
    "\n",
    "    cv=StratifiedKFold(n_splits=3)\n",
    "    cv_results_=cross_validate(model, X, y, cv=cv,\n",
    "                            scoring=['roc_auc',\n",
    "                                    'average_precision',\n",
    "                                    'balanced_accuracy'],\n",
    "                            return_estimator=True)\n",
    "    return pd.DataFrame(cv_results_).agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time                  6.835541\n",
       "score_time                0.149103\n",
       "test_roc_auc              0.910031\n",
       "test_average_precision    0.730929\n",
       "test_balanced_accuracy    0.890110\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_default_res=train_simple_cv(X_transformed, y_train)\n",
    "lr_default_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time                  5.583052\n",
       "score_time                0.156197\n",
       "test_roc_auc              0.959604\n",
       "test_average_precision    0.704659\n",
       "test_balanced_accuracy    0.928720\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cost_sense_res=train_simple_cv(X_transformed, y_train, {0:IR, 1:1})\n",
    "lr_cost_sense_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant boost in ROC AUC and Balanced Accuracy and very slight increase in AP. Hence Cost-Sensitivity helps.\n",
    "\\\n",
    " We can use this technique not only with `LogisticRegression` but other sklearn classifiers. Now let's move on and take a look at resampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling strategies address class imbalance at the data level, by resampling the dataset to reduce the imbalance ratio. The resampling of an imbalanced dataset occurs before the training of the prediction model and can be seen as a data preprocessing step. Let's look at resampling methods one-by-one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling techniques aim at rebalancing the dataset by creating new samples for the minority class. The two most widely-used methods are random oversampling and SMOTE. I'll use the latter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE ([Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)) oversamples the minority class by generating synthetic examples in the neighborhood of observed ones. The idea is to form new minority examples by interpolating between samples of the same class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv(model, X, y, splits=5):\n",
    "    cv=StratifiedKFold(n_splits=splits)\n",
    "    cv_results_=cross_validate(model, X, y, cv=cv,\n",
    "                            scoring=['roc_auc',\n",
    "                                    'average_precision',\n",
    "                                    'balanced_accuracy'],\n",
    "                            return_estimator=True,\n",
    "                            n_jobs=-1)\n",
    "    return pd.DataFrame(cv_results_).agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_smote=imblearn.pipeline.Pipeline([('smote', SMOTE()), ('model', LogisticRegression())])\n",
    "smote_res=train_cv(model_smote, X_transformed, y_train)\n",
    "smote_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really good results here. There are a couple of modified versions of vanilla SMOTE, like SVM-SMOTE or KMeans-SMOTE. Let's test the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use a simpler resampling method called Random Oversampling. Here we don't generate new samples, but just over-sample the minority class(es) by picking samples at random with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ros=imblearn.pipeline.Pipeline([('ros', RandomOverSampler()), ('model', LogisticRegression())])\n",
    "ros_res=train_cv(model_ros, X_transformed, y_train)\n",
    "ros_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC is almost the same, but AP and Balanced Accuracy did drop a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling refers to the process of reducing the number of samples in the majority class. The naive approach, called random undersampling (RUS), consists in randomly removing samples from the majority class until the desired imbalance ratio is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rus=imblearn.pipeline.Pipeline([('rus', RandomUnderSampler()), ('model', LogisticRegression())])\n",
    "rus_res=train_cv(model_rus, X_transformed, y_train)\n",
    "rus_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually can combine Over and Undersampling techniques. LEt's take a look at SMOTE-RUS combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_smote_rus=imblearn.pipeline.Pipeline([('smote', SMOTE(sampling_strategy=.5)), \n",
    "                                            ('rus', RandomUnderSampler(sampling_strategy=1.0)), \n",
    "                                            ('model', LogisticRegression())])\n",
    "smote_rus_res=train_cv(model_smote_rus, X_transformed, y_train)\n",
    "smote_rus_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods consist in training multiple prediction models for the same prediction task, and in combining their outputs to make the final prediction. There are ways to incorporate cost-sensitivity or resampling methods to ensembles (see [BalancedBaggingClassifier](https://imbalanced-learn.org/dev/references/generated/imblearn.ensemble.BalancedBaggingClassifier.html), [BalancedRandomForestClassifier](https://imbalanced-learn.org/dev/references/generated/imblearn.ensemble.BalancedRandomForestClassifier.html)). Here I'll use one method, Weighted XGBoost. It's basically XGBoost+Cost Sensitivity. We'll compare delault XGBoost with a weighted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_xgb=XGBClassifier()\n",
    "default_xgb_res=train_cv(default_xgb, X_transformed, y_train)\n",
    "default_xgb_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wighted_xgb=XGBClassifier(scale_pos_weight=1/IR)\n",
    "wighted_xgb_res=train_cv(wighted_xgb, X_transformed, y_train)\n",
    "wighted_xgb_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a slight boost in Balanced Accuracy, ROC AUC and AP are almost the same. Let's now put all results side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp=pd.DataFrame(pd.concat([lr_default_res,\n",
    "            lr_cost_sense_res,\n",
    "            smote_res,\n",
    "            ros_res,\n",
    "            rus_res,\n",
    "            smote_rus_res,\n",
    "            default_xgb_res,\n",
    "            wighted_xgb_res], axis=1))\n",
    "df_comp.columns=['LR', 'Weighted LR', 'SMOTE LR', 'ROS LR', 'RUS LR', 'SMOTE-RUS LR', 'XGBoost', 'Weighted XGBoost']\n",
    "df_comp.index=['Fit Time', 'Score Time', 'ROC AUC', 'Average Precision', 'Balanced Accuracy']\n",
    "df_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last section of the project. Here I'll finalize the model, train it on a full dataset and save it for future usage.\n",
    "\\\n",
    "(NOTE: Hyperparameters were chosen by hand. Hyperparameter optimisation couyld be done, but it'd take A LOT (approx. 15 hours) of time, hence I had to find somewhat optimal parameters by hand.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.2, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.2, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.2, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params={'max_depth':10,\n",
    "        'learning_rate':0.2,\n",
    "        'subsample':0.7,\n",
    "        'colsample_bytree':0.6,\n",
    "        'n_estimators':500,\n",
    "        'scale_pos_weight':1/IR}\n",
    "\n",
    "xgb_clf=XGBClassifier()\n",
    "xgb_clf.set_params(**params)\n",
    "xgb_clf.fit(X_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROC AUC': 0.906454824055268,\n",
       " 'Average Precision': 0.7601437069531122,\n",
       " 'Balanced Accuracy': 0.9064548240552679}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_model(model, X, y):\n",
    "    data_pipe_loaded=joblib.load('../models/data_pipe.joblib')\n",
    "    X_trans=data_pipe_loaded.transform(X)\n",
    "    roc_auc=roc_auc_score(y, model.predict(X_trans))\n",
    "    average_precision=average_precision_score(y, model.predict(X_trans))\n",
    "    balanced_accuracy=balanced_accuracy_score(y, model.predict(X_trans))\n",
    "\n",
    "    return {'ROC AUC': roc_auc,\n",
    "            'Average Precision': average_precision, \n",
    "            'Balanced Accuracy': balanced_accuracy}\n",
    "\n",
    "final_res=eval_model(xgb_clf, X_test, y_test)\n",
    "final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll create and save a final model that will be trained on an entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.6, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.6, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.6, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, ...)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf_final=XGBClassifier()\n",
    "xgb_clf_final.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((284807, 347), (284807,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipe_loaded=joblib.load('../models/data_pipe.joblib')\n",
    "X_final=data_pipe_loaded.transform(X)\n",
    "\n",
    "X_final.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.2, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.2, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.2, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf_final.fit(X_final, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/xgb_final_model.sav']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_clf_final, '../models/xgb_final_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all! In this project we've seen several ways to deal with imbalanced data. Hope you've learned something new and found the project helpful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
